---
title: "Homework 6 - EM"
author: "Qinghua Li"
date: "3/17/2025"
output: html_document
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
include-before:
- '\newcommand{\bfm}[1]{\ensuremath{\mathbf{#1}}}'
- '\newcommand{\bdm}[1]{\ensuremath{\boldsymbol{#1}}}'
- '$\def \d \bfm{d}$'
- '$\def \e \bfm{e}$'
- '$\def \g \bfm{g}$'
- '$\def \I \bfm{I}$'
- '$\def \l \bfm{l}$'
- '$\def \M \bfm{M}$'
- '$\def \W \bfm{W}$'
- '$\def \y \bfm{y}$'
- '$\def \Y \bfm{Y}$'
- '$\def \x \bfm{x}$'
- '$\def \X \bfm{X}$'
- '$\def \z \bfm{z}$'
- '$\def \thetab \boldsymbol{\theta}$'
- '$\def \betab \boldsymbol{\beta}$'
- '$\def \pib \boldsymbol{\pi}$'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Question 1:  Not So Simple Univariate Optimization

Let us revisit the problem from the last HW, now using BFGS to fit the model.  Report the results of the various starting values as last time, and comment on the convergence for each of the starting values relative the last HW that uses NR.  What properties about BFGS relative to NR could explain the different behavior in this setting? 

$$f(x) = 1.95 - e^{-2/x} - 2e^{-x^4}.$$

```{r}
# f(x)
f = function(x){
  ## solution
 1.95 - exp(-2/x) - 2*exp(-x^4)
  ## end solution
}

# first derivative
f1 = function(x){
  ## solution
 -2*exp(-2/x)/x^2 + 8*exp(-x^4)*x^3
  ## end solution
}


# to start the model, can use maxit/tolerance defaults from optimx
x = 1.2 # also try 0.5 and 0.99



## solution
library(optimx)
fit <- optimx(
  par = x,     
  fn = f,               
  gr = f1,              # 1st derivative
  method = "BFGS",
  
  control = list(
    trace = 0,          # higher number print more detailed output
    maximize = TRUE     # default is to minimize
  )
)

print(fit)
## end solution
```

* x = 0.5
```{r}
x = 0.5
fit <- optimx(
  par = x,     
  fn = f,               
  gr = f1,              # 1st derivative
  method = "BFGS",
  
  control = list(
    trace = 0,          # higher number print more detailed output
    maximize = TRUE     # default is to minimize
  )
)

print(fit)
```

* x = 0.99
```{r}
x = 0.99
fit <- optimx(
  par = x,     
  fn = f,               
  gr = f1,              # 1st derivative
  method = "BFGS",
  
  control = list(
    trace = 0,          # higher number print more detailed output
    maximize = TRUE     # default is to minimize
  )
)

print(fit)
```

* Recall that from HW5 with NR, the results diverged for x = 0.5 and x = 0.99. Using BFGS, we see some robustness against divergence. 


## EM:  Zero-inflated Poisson 

Revisiting problem 3 from HW5, let us implement an EM-based maximization approach to estimate the model parameters.

Please define the CDLL, E-step, and M-step below as we did in class.   

Then, fill in the relevant portions of the code below. 

Hint for writing the CDLL:  Let $z_i = 1$ represent the true (known) membership to the non-fishing population, and $z_i = 0$ to represent membership to the fishing population.  Start with defining the complete data likelihood based on the non-aggregated likelihood below, then take the log to get the final CDLL form. This will help derive the forms for the E and M-steps.  For the actual fitting, we give some direction in the code below in terms of how to use the table aggregated data by a weighting approach. 

### Expression for Log Likelihood: from the previous HW

Lets rewrite the likelihood for the aggregated form of the data in terms of what it would look like when using the $n$ raw, non-aggregated responses:

$$ 
L(\boldsymbol{\theta}) = \prod_{i=1}^n (\pi + (1-\pi)e^{-\lambda})^{I[y_i=0]}\left((1-\pi)\frac{e^{-\lambda}\lambda^{y_i}}{y_i!}\right)^{I[y_i>0]}
$$

This is a simplified form of the PMF that was given at the beginning of the EM lecture. This corresponds to the following log-likelihood

$$
\mathcal{l}(\boldsymbol{\theta}) = \sum_{i=1}^n I[y_i=0]\log(\pi + (1-\pi)e^{-\lambda}) + I[y_i>0]\left(\log(1-\pi) -\lambda + {y_i}\log(\lambda) + \log{y_i!}\right)
$$

Therefore, if $y > 0$, we know automatically that that individual is from the fishing population.    


### Expression for Complete Data Log Likelihood: Solution

Start with the CDL
$$ 
L(\boldsymbol{\theta}) = \prod_{i=1}^n (\pi^{z_i}([(1-\pi)e^{-\lambda}]^{1-z_i})^{I[y_i=0]}\left((1-\pi)\frac{e^{-\lambda}\lambda^{y_i}}{y_i!}\right)^{I[y_i>0](1-z_i)}
$$

Now take the log

$$
\mathcal{l}(\boldsymbol{\theta}) = \sum_{i=1}^n z_i*I[y_i=0]*\log(\pi + (1-\pi)e^{-\lambda}) + (1-z_i)*\left(\log(1-\pi) -\lambda + {y_i}\log(\lambda) + \log{y_i!}\right)
$$


### Expression for E-step: Solution

$$
\begin{align}
Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}}) &= E(l(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}},\boldsymbol{y_i})) \\
&= \sum_{i=1}^n E(z_i|\boldsymbol{\theta^t}, \boldsymbol{y_i})*I(y_i=0)*log(\pi^t) + (1-E(z_i|\boldsymbol{\theta^{(t)}}, \boldsymbol{y_i}))*\left(\log(1-\pi^{(t)}) -\lambda^{(t)} + {y_i}\log(\lambda^{(t)}) + \log{y_i!}\right)
\end{align}
$$
where

$$
\begin{align}
E(z_i|\boldsymbol{\theta^{(t)}, y_i}) & = p(z_i = 1|\boldsymbol{\theta^{(t)}, y_i}) \\
&= \frac{p(z_i=1, \boldsymbol{y_i}, \boldsymbol{\theta^{(t)}})}{p(\boldsymbol{y_i}|\boldsymbol{\theta^{(t)}})}\\
&= \frac{\pi^{(t)} I[y_i=0]}{\pi^{(t)} I[y_i=0]+(1-\pi^{(t)})e^{-\lambda^{(t)}}}
\end{align}
$$


### Expression for M-step: Solution
M-step is to maxmize the parameters, we take partial derivative w.r.t $\pi$ and $\lambda$ respectively, 

$$
\begin{align} 
\frac{\partial Q(\boldsymbol{\theta}|\boldsymbol{\theta^t})}{\partial\pi} = \sum_{i=1}^n E(z_i|\boldsymbol{\theta^{(t)}, y_i}){\frac{1}{\pi^{(t)}}}-\sum_{i=1}^n (1-E(z_i|\boldsymbol{\theta^{(t)}, y_i})){\frac{1}{1-\pi^{(t)}}}= 0
\end{align}
$$

$$
\begin{align}
\frac{\partial Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}})}{\partial\lambda} = \sum_{i=1}^n(1-E(z_i|\boldsymbol{\theta^{(t)}, y_i}))(-1+\frac{y_i}{\lambda^{(t)}}) = 0
\end{align}
$$

Solve the equations above, we get
$$
\hat{\pi}^{(t+1)} = \frac{\sum_{i=1}^n E(z_i|\boldsymbol{\theta^{(t)}, y_i})}{n}
$$

$$
\hat{\lambda}^{t+1} = \frac{\sum_{i=1}^n (1- E(z_i|\boldsymbol{\theta^{(t)}, y_i})){y_i}}{\sum_{i=1}^n (1- E(z_i|\boldsymbol{\theta^{(t)}, y_i}))}
$$



### Code implementation 

```{r}
# data 
y = 0:6
ny = c(3062, 587, 284, 103, 33, 4, 2)

## HINT:  to adjust using relative freq of counts in model/calculations when using aggregated data 
y_weight = ny/sum(ny) 
## For example
print(sum(ny*y)/sum(ny)) # mean of y based on aggregated data in table
## We get the same thing when fitting and intercept only poisson reg model, adjusting for relative freq of counts...
print(exp(glm(y ~ 1, weight = y_weight)$coef))

# to start the model
tol = 10^-8
maxit = 50
iter = 0
eps = Inf
ll = -10000

## create posterior probability matrix
pp = matrix(0,length(y), 2)
colnames(pp) = c("non-fisher", "fisher")

## initialize partion, everything  with count 0 is non-fisher, otherwise fisher
pp[which(y == 0),1] = 1
pp[,2] = 1 - pp[,1]

## now start the EM algorithm
while(eps > tol & iter < maxit){
  
  ## save old ll
    ll0 = ll
  
  ## start M-step
    # pi, 1 x 2 vector
    pi = colSums(pp*y_weight)
    
    # lambda, scalar
    fit = glm(y ~ 1, family = poisson(), weights = pp[,2]*y_weight)
    lambda = exp(fit$coef)
  
  ## start E-step
    # update pp
    pp[1, 1] = pi[1]/(pi[1] + pi[2] * exp(-lambda))
    pp[2, 1] = pp[3, 1] = pp[4, 1] = pp[5, 1] = pp[6, 1] = pp[7, 1] = 0
    pp[,2] = 1 - pp[,1]
    
  ## calculate LL
    ll = sum(log(pi[1]*(y == 0) + pi[2]*dpois(y,lambda = fit$fitted))*ny)
      
  ## calculate relative change in log likelihood  
    eps  = abs(ll-ll0)/abs(ll0)
  
  ## update iterator
    iter = iter + 1
    if(iter == maxit) warning("Iteration limit reached without convergence")
  
  ## print out info to keep track
    cat(sprintf("Iter: %d logL: %.2f pi1: %.3f  eps:%f\n",iter, ll,pi[1],eps))
}
```

```{r}
print(pi)
```

```{r}
cat("fitted lambda = ", lambda)
```

